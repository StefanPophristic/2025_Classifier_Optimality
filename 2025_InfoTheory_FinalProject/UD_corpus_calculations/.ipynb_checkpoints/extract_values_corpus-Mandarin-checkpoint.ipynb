{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chinese Noun and MW Script\n",
    "Boxuan Li + Stefan Pophristic \n",
    "April 2025\n",
    "\n",
    "I am making edits in this one, will merge with the original later. -Stefan\n",
    "\n",
    "Generate a csv with all combos of classifiers and nouns in the corpus. \n",
    "\n",
    "Input: link to corpus\n",
    "\n",
    "Output: \n",
    "- chinese_noun_mw.csv \n",
    "    - Noun: (string) Noun used\n",
    "    - MW: (string) measure word used\n",
    "    - Count_Pre: (int) Number of times a given measure word appears before a noun in the corpus. \n",
    "    - Count_Post: (int) Number of times a given measure word appears after a noun in the corpus.\n",
    "- chinese_all_nouns.csv\n",
    "    - Noun: (string) Noun\n",
    "    - Count: (int) Number of times a given noun appears in the corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import tarfile\n",
    "import io\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# For displaying Chinese characters properly\n",
    "import matplotlib\n",
    "matplotlib.rcParams['font.family'] = ['Arial Unicode MS', 'SimHei', 'sans-serif']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up some configuration variables\n",
    "OUTPUT_CSV = '/Volumes/Server/SHARED/Corpora/Universal_Dependencies/2025_InformationTheory_Project/chinese_noun_mw.csv'\n",
    "CORPUS_URL = \"https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-4923/ud-treebanks-v2.12.tgz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in & Format Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 Chinese GSDSimp files.\n",
      "/Volumes/Server/SHARED/Corpora/Universal_Dependencies/2025_InformationTheory_Project/Universal Dependencies 2.15/ud-treebanks-v2.15/UD_Chinese-GSDSimp/zh_gsdsimp-ud-test.conllu\n",
      "/Volumes/Server/SHARED/Corpora/Universal_Dependencies/2025_InformationTheory_Project/Universal Dependencies 2.15/ud-treebanks-v2.15/UD_Chinese-GSDSimp/zh_gsdsimp-ud-dev.conllu\n",
      "/Volumes/Server/SHARED/Corpora/Universal_Dependencies/2025_InformationTheory_Project/Universal Dependencies 2.15/ud-treebanks-v2.15/UD_Chinese-GSDSimp/zh_gsdsimp-ud-train.conllu\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Finds only Chinese GSDSimp files in the specified directory.\n",
    "\n",
    "Input: \n",
    "    - base_dir (string): Directory where the treebanks were extracted\n",
    "Output: \n",
    "    - List of file paths to Chinese GSDSimp CoNLL-U files\n",
    "\"\"\"\n",
    "def get_chinese_ud_files(base_dir):\n",
    "    gsdsimp_files = []\n",
    "    \n",
    "    # Walk through directories looking for GSDSimp CoNLL-U files\n",
    "    for root, dirs, files in os.walk(base_dir):\n",
    "        if \"UD_Chinese-GSDSimp\" in root:\n",
    "            for file in files:\n",
    "                if file.endswith(\".conllu\"):\n",
    "                    gsdsimp_files.append(os.path.join(root, file))\n",
    "    \n",
    "    print(f\"Found {len(gsdsimp_files)} Chinese GSDSimp files.\")\n",
    "    return gsdsimp_files\n",
    "\n",
    "# Now call the function with the actual path to your extracted files\n",
    "# Replace this path with where you actually extracted the files\n",
    "base_directory = '/Volumes/Server/SHARED/Corpora/Universal_Dependencies/2025_InformationTheory_Project/Universal Dependencies 2.15/ud-treebanks-v2.15'\n",
    "chinese_files = get_chinese_ud_files(base_directory)\n",
    "\n",
    "for i in chinese_files:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 3. Parsing CoNLL-U Format\n",
    "# \n",
    "# The corpus is in CoNLL-U format, which contains 10 columns of linguistic information.\n",
    "# We'll parse this format into a more structured representation for analysis.\n",
    "\n",
    "# %%\n",
    "def parse_conllu_sentences(file_path):\n",
    "    \"\"\"\n",
    "    Parses a CoNLL-U format file into structured sentence objects with dependency information.\n",
    "    \n",
    "    Input: file_path (string) - Path to a CoNLL-U format file\n",
    "    Output: List of sentence dictionaries, where each dictionary contains:\n",
    "            - 'id': sentence ID\n",
    "            - 'tokens': list of token dictionaries with linguistic annotations\n",
    "            - 'token_dict': dictionary mapping token IDs to token objects for easy lookup\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    current_sentence = []\n",
    "    sent_id = None\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            \n",
    "            if line.startswith('# sent_id ='):\n",
    "                sent_id = line.split('=')[1].strip()\n",
    "            elif line.startswith('#'):\n",
    "                continue\n",
    "            elif not line:\n",
    "                if current_sentence:\n",
    "                    # Build token lookup by ID for easy reference\n",
    "                    token_dict = {token['id']: token for token in current_sentence}\n",
    "                    \n",
    "                    # Link tokens based on dependencies\n",
    "                    for token in current_sentence:\n",
    "                        token['head_token'] = token_dict.get(token['head'], None)\n",
    "                    \n",
    "                    sentences.append({\n",
    "                        'id': sent_id,\n",
    "                        'tokens': current_sentence,\n",
    "                        'token_dict': token_dict\n",
    "                    })\n",
    "                    current_sentence = []\n",
    "                    sent_id = None\n",
    "                continue\n",
    "            \n",
    "            fields = line.split('\\t')\n",
    "            if len(fields) == 10 and '-' not in fields[0]:\n",
    "                token = {\n",
    "                    'id': fields[0],\n",
    "                    'form': fields[1],\n",
    "                    'lemma': fields[2],\n",
    "                    'upos': fields[3],\n",
    "                    'xpos': fields[4],\n",
    "                    'feats': fields[5],\n",
    "                    'head': fields[6],  # ID of the head token\n",
    "                    'deprel': fields[7], # Dependency relation\n",
    "                    'deps': fields[8],\n",
    "                    'misc': fields[9],\n",
    "                    'head_token': None  # Will be populated after parsing\n",
    "                }\n",
    "                current_sentence.append(token)\n",
    "    \n",
    "    if current_sentence:\n",
    "        token_dict = {token['id']: token for token in current_sentence}\n",
    "        for token in current_sentence:\n",
    "            token['head_token'] = token_dict.get(token['head'], None)\n",
    "        \n",
    "        sentences.append({\n",
    "            'id': sent_id,\n",
    "            'tokens': current_sentence,\n",
    "            'token_dict': token_dict\n",
    "        })\n",
    "    \n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "500\n",
      "3997\n"
     ]
    }
   ],
   "source": [
    "# sample_file = chinese_files[0] # Take first file as sample\n",
    "for i in chinese_files:\n",
    "    sample_sentences = parse_conllu_sentences(i)\n",
    "    print(len(sample_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3997"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sample_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 4997 sentences from 3\n"
     ]
    }
   ],
   "source": [
    "# Parse all files for the sentences\n",
    "\n",
    "sample_sentences = []\n",
    "\n",
    "if chinese_files:\n",
    "    for file in chinese_files:\n",
    "        \n",
    "        sample_sentences = sample_sentences + parse_conllu_sentences(file)\n",
    "    \n",
    "print(f\"Parsed {len(sample_sentences)} sentences from {len(chinese_files)}\")\n",
    "\n",
    "all_sentences = sample_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample sentence structure:\n",
      "Sentence ID: test-s1\n",
      "Number of tokens: 11\n",
      "example sentence: 然而，这样的处理也衍生了一些问题。\n",
      "\n",
      "First 5 tokens:\n",
      "1. Form: '然而', POS: SCONJ, Relation: mark\n",
      "2. Form: '，', POS: PUNCT, Relation: punct\n",
      "3. Form: '这样', POS: PRON, Relation: det\n",
      "4. Form: '的', POS: PART, Relation: case\n",
      "5. Form: '处理', POS: NOUN, Relation: nsubj\n"
     ]
    }
   ],
   "source": [
    "# Display a sample sentence\n",
    "if sample_sentences:\n",
    "    print(\"\\nSample sentence structure:\")\n",
    "    sample_sentence = sample_sentences[0]\n",
    "    print(f\"Sentence ID: {sample_sentence['id']}\")\n",
    "    print(f\"Number of tokens: {len(sample_sentence['tokens'])}\")\n",
    "\n",
    "    ex_sentence = \"\"\n",
    "\n",
    "    for i, token in enumerate(sample_sentence['tokens']):\n",
    "        ex_sentence = ex_sentence + token['form']\n",
    "\n",
    "    print(f\"example sentence: {ex_sentence}\")\n",
    "    # Display first few tokens\n",
    "    print(\"\\nFirst 5 tokens:\")\n",
    "    for i, token in enumerate(sample_sentence['tokens'][:5]):\n",
    "        print(f\"{i+1}. Form: '{token['form']}', POS: {token['upos']}, Relation: {token['deprel']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match at index 10: 圆齿龙（Globidens）意为“球状牙齿”，是沧龙科的一个属。\n"
     ]
    }
   ],
   "source": [
    "# Find first sentence that has a linear combination of two or more characters\n",
    "# e.g. [一][栋], [一这][栋只]\n",
    "sample_sentence = sample_sentences\n",
    "for idx, sample_sentence in enumerate(sample_sentences):\n",
    "\n",
    "    ex_sentence = \"\"\n",
    "\n",
    "    for token in sample_sentence['tokens']:\n",
    "        ex_sentence += token['form']\n",
    "        \n",
    "    if re.search(r'[一二][个]', ex_sentence):\n",
    "        print(f\"Match at index {idx}: {ex_sentence}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found at index 10: 圆齿龙（Globidens）意为“球状牙齿”，是沧龙科的一个属。\n",
      "Found at index 17: 毛泽东早在1949年3月中共七届二中全会的报告中就明确地说：“占国民经济总产值90%的分散的个体的农业经济和手工业经济，是可能和必须谨慎地、逐步地而又积极地引导它们向着现代化和集体化的方向发展的，任其自流的观点是错误的。”\n"
     ]
    }
   ],
   "source": [
    "# Find first example that contains a specific character\n",
    "# skip in case you need another one in the idx if statement \n",
    "\n",
    "for idx, sample_sentence in enumerate(sample_sentences):\n",
    "    ex_sentence = \"\"\n",
    "\n",
    "    for token in sample_sentence['tokens']:\n",
    "        ex_sentence += token['form']\n",
    "            \n",
    "    if '个' in ex_sentence:\n",
    "        print(f\"Found at index {idx}: {ex_sentence}\")\n",
    "        if idx in [10 ]:\n",
    "            continue\n",
    "        break  # Stop once found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sentence 3\n",
    "token 9\n",
    "sentence: 台大医学人文博物馆是一**栋**两层楼的建筑，沿中山南路与仁爱路成L型。\n",
    "Token: 栋\n",
    "\n",
    "{'id': '8', \n",
    " 'form': '栋', **MW Dong (for buildings)**\n",
    " 'lemma': '栋', \n",
    " 'upos': 'NOUN', \n",
    " 'xpos': 'NNB', \n",
    " 'feats': '_', \n",
    " 'head': '7', \n",
    " 'deprel': 'clf', **This is where we get the classifier**\n",
    " 'deps': '_', \n",
    " 'misc': 'SpaceAfter=No|Translit=栋|LTranslit=栋', \n",
    " 'head_token': {'id': '7',  **The measureword is dependent on the numeral \"1\"**\n",
    "                'form': '一', \n",
    "                'lemma': '一', \n",
    "                'upos': 'NUM', \n",
    "                'xpos': 'CD', \n",
    "                'feats': 'NumType=Card', \n",
    "                'head': '13', \n",
    "                'deprel': 'nummod', \n",
    "                'deps': '_', \n",
    "                'misc': 'SpaceAfter=No|Translit=yī|LTranslit=yī', \n",
    "                'head_token': {...}}\n",
    "\n",
    "sentence 10\n",
    "token 17\n",
    "sentence: 圆齿龙（Globidens）意为“球状牙齿”，是沧龙科的一个属。\n",
    "{'id': '17', \n",
    "'form': '个', \n",
    "'lemma': '个', \n",
    "'upos': 'NOUN', \n",
    "'xpos': 'NNB', \n",
    "'feats': '_', \n",
    "'head': '16', \n",
    "'deprel': 'clf', \n",
    "'deps': '_', \n",
    "'misc': 'SpaceAfter=No|Translit=gè|LTranslit=gè', \n",
    "'head_token': {'id': '16', \n",
    "                'form': '一', \n",
    "                'lemma': '一', \n",
    "                'upos': 'NUM', \n",
    "                'xpos': 'CD', '\n",
    "                feats': 'NumType=Card', \n",
    "                'head': '18', \n",
    "                'deprel': 'nummod', \n",
    "                'deps': '_', \n",
    "                'misc': 'SpaceAfter=No|Translit=yī|LTranslit=yī', \n",
    "                'head_token': {...}}}\n",
    "\n",
    "sentence 22\n",
    "token 2\n",
    "sentence: 这个计算机控制了火箭从起飞前一直到抛弃S-IVB推进器的操作过程。\n",
    "token: 个\n",
    "{'id': '2', \n",
    "'form': '个', \n",
    "'lemma': '个', \n",
    "'upos': 'NOUN', \n",
    "'xpos': 'NNB', \n",
    "'feats': '_', \n",
    "'head': '4', \n",
    "'deprel': 'nmod', \n",
    "'deps': '_', \n",
    "'misc': 'SpaceAfter=No|Translit=gè|LTranslit=gè', \n",
    "'head_token': {'id': '4', \n",
    "                'form': '机', \n",
    "                'lemma': '机', \n",
    "                'upos': 'PART', \n",
    "                'xpos': 'SFN', \n",
    "                'feats': '_', \n",
    "                'head': '5', \n",
    "                'deprel': 'nsubj', \n",
    "                'deps': '_', 'misc': 'SpaceAfter=No|Translit=jī|LTranslit=jī', \n",
    "                'head_token': {...}}}\n",
    "\n",
    "sentence 50\n",
    "token 19\n",
    "sentence: 换句话说，如果每个标注的点都在100米的高度，这条线代表的就是100米海拔。\n",
    "\n",
    "{'id': '19', 'form': '条', \n",
    "'lemma': '条', \n",
    "'upos': 'NOUN', \n",
    "'xpos': 'NNB', \n",
    "'feats': '_', \n",
    "'head': '20', \n",
    "'deprel': 'clf', \n",
    "'deps': '_', \n",
    "'misc': 'SpaceAfter=No|Translit=tiáo|LTranslit=tiáo', \n",
    "'head_token': {'id': '20', \n",
    "               'form': '线', \n",
    "               'lemma': '线', \n",
    "               'upos': 'NOUN', \n",
    "               'xpos': 'NN', \n",
    "               'feats': '_', \n",
    "               'head': '21', \n",
    "               'deprel': 'nsubj', \n",
    "               'deps': '_', \n",
    "               'misc': 'SpaceAfter=No|Translit=xiàn|LTranslit=xiàn', \n",
    "               'head_token': {....}}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifiers are always marked as nouns for universal part of speech (upos). \n",
    "They differ in their classification under a dependency relation (deprel).\n",
    "\n",
    "In the case where a MW appears before a numeral: \n",
    "- deprel: \"clf\" (classifier). They are marked as dependent on the numeral.\n",
    "- This is regardless of whether it is 个 or not\n",
    "\n",
    "MW + Demonstrative:\n",
    "- If specific MW: deprel: \"clf\" (classifier). They are marked as dependent on the noun.\n",
    "- If 个: deprel: \"nmod\" (nominal modifier). Marked as dependent on the noun. \n",
    "\n",
    "MW + Question word (哪) not present in corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '8',\n",
       " 'form': '栋',\n",
       " 'lemma': '栋',\n",
       " 'upos': 'NOUN',\n",
       " 'xpos': 'NNB',\n",
       " 'feats': '_',\n",
       " 'head': '7',\n",
       " 'deprel': 'clf',\n",
       " 'deps': '_',\n",
       " 'misc': 'SpaceAfter=No|Translit=栋|LTranslit=栋',\n",
       " 'head_token': {'id': '7',\n",
       "  'form': '一',\n",
       "  'lemma': '一',\n",
       "  'upos': 'NUM',\n",
       "  'xpos': 'CD',\n",
       "  'feats': 'NumType=Card',\n",
       "  'head': '13',\n",
       "  'deprel': 'nummod',\n",
       "  'deps': '_',\n",
       "  'misc': 'SpaceAfter=No|Translit=yī|LTranslit=yī',\n",
       "  'head_token': {'id': '13',\n",
       "   'form': '建筑',\n",
       "   'lemma': '建筑',\n",
       "   'upos': 'NOUN',\n",
       "   'xpos': 'NN',\n",
       "   'feats': '_',\n",
       "   'head': '22',\n",
       "   'deprel': 'advcl',\n",
       "   'deps': '_',\n",
       "   'misc': 'SpaceAfter=No|Translit=jiànzhù|LTranslit=jiànzhù',\n",
       "   'head_token': {'id': '22',\n",
       "    'form': '成',\n",
       "    'lemma': '成',\n",
       "    'upos': 'VERB',\n",
       "    'xpos': 'VV',\n",
       "    'feats': '_',\n",
       "    'head': '0',\n",
       "    'deprel': 'root',\n",
       "    'deps': '_',\n",
       "    'misc': 'SpaceAfter=No|Translit=chéng|LTranslit=chéng',\n",
       "    'head_token': None}}}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence = sample_sentences[3]\n",
    "# print(test_sentence)\n",
    "test_token = test_sentence['tokens'][7]\n",
    "test_token\n",
    "# print(test_token['upos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example sentence: 台大医学人文博物馆是一栋两层楼的建筑，沿中山南路与仁爱路成L型。\n"
     ]
    }
   ],
   "source": [
    "# For testing: print the full example sentence\n",
    "\n",
    "sample_sentence = sample_sentences[3]\n",
    "ex_sentence = \"\"\n",
    "\n",
    "for i, token in enumerate(sample_sentence['tokens']):\n",
    "    ex_sentence = ex_sentence + token['form']\n",
    "\n",
    "print(f\"example sentence: {ex_sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Relevant Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 4. Identifying Measure Words\n",
    "# \n",
    "# Now we'll create functions to identify measure words (classifiers) in Chinese.\n",
    "# We'll use multiple criteria including POS tags, dependency relations, and a list of common measure words.\n",
    "\n",
    "# %%\n",
    "\n",
    "\"\"\"\n",
    "Check whether a given token is a measure word\n",
    "\n",
    "Measure words should be marked as a 'upos' noun and have a 'clf' (classifier) relationship to the head\n",
    "\n",
    "Input: token (dict) - A token dictionary with linguistic annotations\n",
    "Output: Boolean - True if the token is a measure word, False otherwise\n",
    "\"\"\"\n",
    "def is_measure_word(token):\n",
    "    # Most tokens will be marked as 'clf'\n",
    "    if token['upos'] == 'NOUN' and token['deprel'] == 'clf':\n",
    "        return True\n",
    "        \n",
    "    # in the case of demonstratives and 个 we get a different marking, so include those cases as well \n",
    "    elif token['form'] == '个':\n",
    "        return True\n",
    "        \n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test numeral + MW, should return true: True\n",
      "Test numeral + 个, should return true: True\n",
      "Test a demonstrative + MW, should return true: True\n",
      "Test a demonstrative + 个, should return true: True\n",
      "Test a non-MW, should return false: False\n"
     ]
    }
   ],
   "source": [
    "test_sentence = sample_sentences[3]\n",
    "test_token = test_sentence['tokens'][7]\n",
    "\n",
    "print(f\"Test numeral + MW, should return true: {is_measure_word(test_token)}\")\n",
    "\n",
    "test_sentence = sample_sentences[10]\n",
    "test_token = test_sentence['tokens'][16]\n",
    "\n",
    "print(f\"Test numeral + 个, should return true: {is_measure_word(test_token)}\")\n",
    "\n",
    "\n",
    "test_sentence = sample_sentences[22]\n",
    "test_token = test_sentence['tokens'][1]\n",
    "\n",
    "print(f\"Test a demonstrative + MW, should return true: {is_measure_word(test_token)}\")\n",
    "\n",
    "\n",
    "test_sentence = sample_sentences[50]\n",
    "test_token = test_sentence['tokens'][18]\n",
    "\n",
    "print(f\"Test a demonstrative + 个, should return true: {is_measure_word(test_token)}\")\n",
    "\n",
    "\n",
    "test_sentence = sample_sentences[50]\n",
    "test_token = test_sentence['tokens'][16]\n",
    "\n",
    "print(f\"Test a non-MW, should return false: {is_measure_word(test_token)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Measure words found in the sample:\n",
      "'个': 632 occurrences\n",
      "'年': 517 occurrences\n",
      "'月': 373 occurrences\n",
      "'种': 129 occurrences\n",
      "'次': 69 occurrences\n",
      "'名': 57 occurrences\n",
      "'条': 51 occurrences\n",
      "'位': 40 occurrences\n",
      "'座': 33 occurrences\n",
      "'部': 31 occurrences\n",
      "'场': 25 occurrences\n",
      "'所': 21 occurrences\n",
      "'段': 20 occurrences\n",
      "'家': 18 occurrences\n",
      "'项': 18 occurrences\n",
      "'世纪': 17 occurrences\n",
      "'颗': 17 occurrences\n",
      "'届': 15 occurrences\n",
      "'间': 15 occurrences\n",
      "'张': 14 occurrences\n",
      "'年代': 12 occurrences\n",
      "'支': 11 occurrences\n",
      "'任': 11 occurrences\n",
      "'批': 11 occurrences\n",
      "'枚': 10 occurrences\n",
      "'件': 8 occurrences\n",
      "'日': 8 occurrences\n",
      "'代': 8 occurrences\n",
      "'篇': 8 occurrences\n",
      "'层': 7 occurrences\n",
      "'套': 7 occurrences\n",
      "'只': 6 occurrences\n",
      "'辆': 6 occurrences\n",
      "'时': 6 occurrences\n",
      "'首': 5 occurrences\n",
      "'道': 5 occurrences\n",
      "'款': 5 occurrences\n",
      "'米': 4 occurrences\n",
      "'番': 4 occurrences\n",
      "'句': 4 occurrences\n",
      "'岁': 4 occurrences\n",
      "'台': 4 occurrences\n",
      "'期': 4 occurrences\n",
      "'份': 4 occurrences\n",
      "'艘': 4 occurrences\n",
      "'组': 4 occurrences\n",
      "'季': 3 occurrences\n",
      "'块': 3 occurrences\n",
      "'起': 3 occurrences\n",
      "'类': 3 occurrences\n",
      "'重': 3 occurrences\n",
      "'元': 3 occurrences\n",
      "'桩': 3 occurrences\n",
      "'行': 3 occurrences\n",
      "'本': 3 occurrences\n",
      "'群': 3 occurrences\n",
      "'轮': 3 occurrences\n",
      "'班': 3 occurrences\n",
      "'则': 3 occurrences\n",
      "'片': 3 occurrences\n",
      "'尊': 3 occurrences\n",
      "'栋': 2 occurrences\n",
      "'幢': 2 occurrences\n",
      "'棵': 2 occurrences\n",
      "'世': 2 occurrences\n",
      "'对': 2 occurrences\n",
      "'幅': 2 occurrences\n",
      "'笔': 2 occurrences\n",
      "'回': 2 occurrences\n",
      "'夸脱': 2 occurrences\n",
      "'磅': 2 occurrences\n",
      "'周年': 1 occurrences\n",
      "'处': 1 occurrences\n",
      "'平方呎': 1 occurrences\n",
      "'小段': 1 occurrences\n",
      "'尺': 1 occurrences\n",
      "'级': 1 occurrences\n",
      "'纪元': 1 occurrences\n",
      "'周': 1 occurrences\n",
      "'宗': 1 occurrences\n",
      "'匹': 1 occurrences\n",
      "'册': 1 occurrences\n",
      "'小片': 1 occurrences\n",
      "'里': 1 occurrences\n",
      "'分': 1 occurrences\n",
      "'封': 1 occurrences\n",
      "'手': 1 occurrences\n",
      "'公里': 1 occurrences\n",
      "'吨': 1 occurrences\n",
      "'座座': 1 occurrences\n",
      "'方': 1 occurrences\n",
      "'通': 1 occurrences\n",
      "'连串': 1 occurrences\n",
      "'壶': 1 occurrences\n",
      "'面': 1 occurrences\n",
      "'毫米': 1 occurrences\n",
      "'层级': 1 occurrences\n",
      "'呎': 1 occurrences\n",
      "'版': 1 occurrences\n",
      "'档': 1 occurrences\n",
      "'挺': 1 occurrences\n",
      "'架': 1 occurrences\n",
      "'天': 1 occurrences\n",
      "'集': 1 occurrences\n",
      "'记': 1 occurrences\n",
      "'帮': 1 occurrences\n",
      "'派': 1 occurrences\n",
      "'个个': 1 occurrences\n",
      "'把': 1 occurrences\n",
      "'口': 1 occurrences\n",
      "'双': 1 occurrences\n",
      "'局': 1 occurrences\n"
     ]
    }
   ],
   "source": [
    "# Let's find and count all measure words in our sample\n",
    "if sample_sentences:\n",
    "    measure_words = {}\n",
    "    \n",
    "    for sentence in sample_sentences:\n",
    "        for token in sentence['tokens']:\n",
    "            if is_measure_word(token):\n",
    "                form = token['form']\n",
    "                if form in measure_words:\n",
    "                    measure_words[form] += 1\n",
    "                else:\n",
    "                    measure_words[form] = 1\n",
    "    \n",
    "    print(\"Measure words found in the sample:\")\n",
    "    for mw, count in sorted(measure_words.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"'{mw}': {count} occurrences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My code differs from Boxuan's here in the output. Her's had 本 as a MW with 1 occurance, mine does not it seems. Double check this example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_related_measure_words(token):\n",
    "\n",
    "    if test_token[\"deprel\"] == 'clf':\n",
    "        if test_token[\"head_token\"][\"deprel\"] == 'nummod':\n",
    "            return(test_token[\"head_token\"][\"head_token\"][\"lemma\"])\n",
    "            \n",
    "        elif test_token[\"head_token\"][\"upos\"] == \"NOUN\":\n",
    "            return(test_token[\"head_token\"][\"lemma\"])\n",
    "\n",
    "    elif (test_token[\"deprel\"] == 'nmod') and (test_token[\"lemma\"] == '个'):\n",
    "        return(test_token[\"head_token\"][\"lemma\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence: 台大医学人文博物馆是一栋两层楼的建筑，沿中山南路与仁爱路成L型。\n",
      "MW: 栋\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'建筑'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence = sample_sentences[3]\n",
    "test_token = test_sentence['tokens'][7]\n",
    "\n",
    "ex_sentence = \"\"\n",
    "\n",
    "for token in test_sentence[\"tokens\"]:\n",
    "    ex_sentence += token['lemma']\n",
    "\n",
    "print(f\"sentence: {ex_sentence}\")\n",
    "print(f\"MW: \" + test_token[\"form\"])\n",
    "\n",
    "find_related_measure_words(test_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence: 圆齿龙（Globidens）意为“球状牙齿”，是沧龙科的一个属。\n",
      "MW: 个\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'属'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence = sample_sentences[10]\n",
    "test_token = test_sentence['tokens'][16]\n",
    "\n",
    "ex_sentence = \"\"\n",
    "\n",
    "for token in test_sentence[\"tokens\"]:\n",
    "    ex_sentence += token['lemma']\n",
    "\n",
    "print(f\"sentence: {ex_sentence}\")\n",
    "print(f\"MW: \" + test_token[\"form\"])\n",
    "\n",
    "find_related_measure_words(test_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence: 这个计算机控制了火箭从起飞前一直到抛弃S-IVB推进器的操作过程。\n",
      "MW: 个\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'机'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence = sample_sentences[22]\n",
    "test_token = test_sentence['tokens'][1]\n",
    "\n",
    "ex_sentence = \"\"\n",
    "\n",
    "for token in test_sentence[\"tokens\"]:\n",
    "    ex_sentence += token['lemma']\n",
    "\n",
    "print(f\"sentence: {ex_sentence}\")\n",
    "print(f\"MW: \" + test_token[\"form\"])\n",
    "\n",
    "find_related_measure_words(test_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence: 换句话说，如果每个标注的点都在100米的高度，这条线代表的是100米海拔。\n",
      "MW: 条\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'线'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence = sample_sentences[50]\n",
    "test_token = test_sentence['tokens'][18]\n",
    "\n",
    "ex_sentence = \"\"\n",
    "\n",
    "for token in test_sentence[\"tokens\"]:\n",
    "    ex_sentence += token['lemma']\n",
    "\n",
    "print(f\"sentence: {ex_sentence}\")\n",
    "print(f\"MW: \" + test_token[\"form\"])\n",
    "\n",
    "find_related_measure_words(test_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence: 换句话说，如果每个标注的点都在100米的高度，这条线代表的是100米海拔。\n",
      "MW: 高度\n"
     ]
    }
   ],
   "source": [
    "test_sentence = sample_sentences[50]\n",
    "test_token = test_sentence['tokens'][15]\n",
    "\n",
    "ex_sentence = \"\"\n",
    "\n",
    "for token in test_sentence[\"tokens\"]:\n",
    "    ex_sentence += token['lemma']\n",
    "\n",
    "print(f\"sentence: {ex_sentence}\")\n",
    "print(f\"MW: \" + test_token[\"form\"])\n",
    "\n",
    "find_related_measure_words(test_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 examples of noun-measure word relationships:\n",
      "\n",
      "Example 1:\n",
      "Noun: 栋\n",
      "Pre-noun measure words: None\n",
      "Post-noun measure words: 栋\n",
      "\n",
      "Example 2:\n",
      "Noun: 层\n",
      "Pre-noun measure words: None\n",
      "Post-noun measure words: 层\n",
      "\n",
      "Example 3:\n",
      "Noun: 楼\n",
      "Pre-noun measure words: 层\n",
      "Post-noun measure words: None\n",
      "\n",
      "Example 4:\n",
      "Noun: 建筑\n",
      "Pre-noun measure words: 栋\n",
      "Post-noun measure words: None\n",
      "\n",
      "Example 5:\n",
      "Noun: 天文\n",
      "Pre-noun measure words: None\n",
      "Post-noun measure words: 台\n",
      "\n",
      "Example 6:\n",
      "Noun: 现\n",
      "Pre-noun measure words: 台\n",
      "Post-noun measure words: None\n"
     ]
    }
   ],
   "source": [
    "# ## 5. Finding Noun-Measure Word Relationships\n",
    "# \n",
    "# We'll use dependency relations to find measure words related to nouns, regardless of their position in the sentence.\n",
    "\n",
    "# %%\n",
    "def find_related_measure_words(sentence, noun_token):\n",
    "    \"\"\"\n",
    "    Finds measure words related to a specific noun using dependency relations.\n",
    "    \n",
    "    Input: \n",
    "        - sentence (dict): A sentence dictionary containing tokens and dependency information\n",
    "        - noun_token (dict): The token dictionary for the noun we're analyzing\n",
    "    \n",
    "    Output: Tuple of two lists:\n",
    "        - pre_mws: List of measure word tokens that appear before the noun\n",
    "        - post_mws: List of measure word tokens that appear after the noun\n",
    "    \"\"\"\n",
    "    pre_mws = []\n",
    "    post_mws = []\n",
    "    noun_id = int(noun_token['id'])\n",
    "    \n",
    "    for token in sentence['tokens']:\n",
    "        if is_measure_word(token):\n",
    "            mw_id = int(token['id'])\n",
    "            \n",
    "            # Check if the measure word is directly related to the noun\n",
    "            # This handles cases where the measure word and noun have a direct dependency\n",
    "            if token['head'] == noun_token['id'] or noun_token['head'] == token['id']:\n",
    "                if mw_id < noun_id:\n",
    "                    pre_mws.append(token)\n",
    "                else:\n",
    "                    post_mws.append(token)\n",
    "                continue\n",
    "                \n",
    "            # Check if they share a common head (like a numeral)\n",
    "            # This handles cases like \"三个苹果\" where both \"个\" and \"苹果\" depend on \"三\"\n",
    "            if token['head'] == noun_token['head'] and token['head'] != '0':\n",
    "                if mw_id < noun_id:\n",
    "                    pre_mws.append(token)\n",
    "                else:\n",
    "                    post_mws.append(token)\n",
    "                continue\n",
    "                \n",
    "            # Check if the measure word is connected to a determiner or numeral that connects to the noun\n",
    "            # This handles cases with more complex structures\n",
    "            if token['head_token'] and token['head_token']['head'] == noun_token['id']:\n",
    "                if mw_id < noun_id:\n",
    "                    pre_mws.append(token)\n",
    "                else:\n",
    "                    post_mws.append(token)\n",
    "                continue\n",
    "\n",
    "            # Check if the numeral connects to the measure word connects to the noun\n",
    "            # This handles another common structure\n",
    "            if noun_token['head_token'] and noun_token['head_token']['head'] == token['id']:\n",
    "                if mw_id < noun_id:\n",
    "                    pre_mws.append(token)\n",
    "                else:\n",
    "                    post_mws.append(token)\n",
    "    \n",
    "    return pre_mws, post_mws\n",
    "\n",
    "# %%\n",
    "# Let's examine some examples of noun-measure word relationships\n",
    "if sample_sentences:\n",
    "    examples = []\n",
    "    \n",
    "    for sentence in sample_sentences[:5]:  # Look at first 5 sentences\n",
    "        for token in sentence['tokens']:\n",
    "            if token['upos'] == 'NOUN':\n",
    "                pre_mws, post_mws = find_related_measure_words(sentence, token)\n",
    "                \n",
    "                if pre_mws or post_mws:\n",
    "                    # Store this as an example\n",
    "                    examples.append({\n",
    "                        'noun': token['form'],\n",
    "                        'pre_mws': [t['form'] for t in pre_mws],\n",
    "                        'post_mws': [t['form'] for t in post_mws]\n",
    "                    })\n",
    "    \n",
    "    print(f\"Found {len(examples)} examples of noun-measure word relationships:\")\n",
    "    for i, example in enumerate(examples[:10]):  # Show first 10 examples\n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(f\"Noun: {example['noun']}\")\n",
    "        print(f\"Pre-noun measure words: {', '.join(example['pre_mws']) if example['pre_mws'] else 'None'}\")\n",
    "        print(f\"Post-noun measure words: {', '.join(example['post_mws']) if example['post_mws'] else 'None'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1705 unique nouns in the sample\n",
      "\n",
      "Sample of noun-measure word pairs:\n",
      "\n",
      "Noun: 处理\n",
      "\n",
      "Noun: 问题\n",
      "\n",
      "Noun: 年\n",
      "  Measure word: 年\n",
      "    Pre-noun count: 0\n",
      "    Post-noun count: 50\n",
      "  Measure word: 位\n",
      "    Pre-noun count: 1\n",
      "    Post-noun count: 0\n",
      "\n",
      "Noun: 大楼\n",
      "  Measure word: 位\n",
      "    Pre-noun count: 1\n",
      "    Post-noun count: 1\n",
      "  Measure word: 幢\n",
      "    Pre-noun count: 1\n",
      "    Post-noun count: 0\n",
      "\n",
      "Noun: 构想\n"
     ]
    }
   ],
   "source": [
    "# ## 6. Collecting and Counting Noun-Measure Word Pairs\n",
    "# \n",
    "# Now we'll process all sentences to collect statistics on noun-measure word pairs.\n",
    "\n",
    "# %%\n",
    "def collect_noun_mw_pairs(sentences):\n",
    "    \"\"\"\n",
    "    Collects all noun-measure word pairs from a list of sentences using dependency relations.\n",
    "    \n",
    "    Input: sentences (list) - List of sentence dictionaries with tokens and dependencies\n",
    "    \n",
    "    Output: defaultdict - A nested dictionary structure:\n",
    "            {noun: {measure_word: {'pre': count, 'post': count}, ...}, ...}\n",
    "            Where 'pre' is the count of the measure word appearing before the noun,\n",
    "            and 'post' is the count of the measure word appearing after the noun.\n",
    "            'NA' is used when no measure word is found.\n",
    "    \"\"\"\n",
    "    noun_mw_data = defaultdict(lambda: defaultdict(lambda: {'pre': 0, 'post': 0}))\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        for token in sentence['tokens']:\n",
    "            # If current token is a noun\n",
    "            if token['upos'] == 'NOUN':\n",
    "                noun = token['form']\n",
    "                pre_mws, post_mws = find_related_measure_words(sentence, token)\n",
    "                \n",
    "                # Count pre-noun measure words\n",
    "                if pre_mws:\n",
    "                    for mw_token in pre_mws:\n",
    "                        noun_mw_data[noun][mw_token['form']]['pre'] += 1\n",
    "                else:\n",
    "                    noun_mw_data[noun]['NA']['pre'] += 1\n",
    "                \n",
    "                # Count post-noun measure words\n",
    "                if post_mws:\n",
    "                    for mw_token in post_mws:\n",
    "                        noun_mw_data[noun][mw_token['form']]['post'] += 1\n",
    "                else:\n",
    "                    noun_mw_data[noun]['NA']['post'] += 1\n",
    "    \n",
    "    return noun_mw_data\n",
    "\n",
    "# %%\n",
    "# Collect noun-measure word pairs from our sample\n",
    "sample_noun_mw_data = collect_noun_mw_pairs(sample_sentences)\n",
    "\n",
    "print(f\"Found {len(sample_noun_mw_data)} unique nouns in the sample\")\n",
    "\n",
    "# Display a few examples\n",
    "print(\"\\nSample of noun-measure word pairs:\")\n",
    "sample_count = 0\n",
    "for noun, mw_dict in sample_noun_mw_data.items():\n",
    "    if sample_count >= 5:\n",
    "        break\n",
    "        \n",
    "    print(f\"\\nNoun: {noun}\")\n",
    "    for mw, counts in mw_dict.items():\n",
    "        if mw != 'NA' and (counts['pre'] > 0 or counts['post'] > 0):\n",
    "            print(f\"  Measure word: {mw}\")\n",
    "            print(f\"    Pre-noun count: {counts['pre']}\")\n",
    "            print(f\"    Post-noun count: {counts['post']}\")\n",
    "    \n",
    "    sample_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /Volumes/Server/SHARED/Corpora/Universal_Dependencies/2025_InformationTheory_Project/Universal Dependencies 2.15/ud-treebanks-v2.15/UD_Chinese-GSDSimp/zh_gsdsimp-ud-test.conllu...\n",
      "Processing /Volumes/Server/SHARED/Corpora/Universal_Dependencies/2025_InformationTheory_Project/Universal Dependencies 2.15/ud-treebanks-v2.15/UD_Chinese-GSDSimp/zh_gsdsimp-ud-dev.conllu...\n",
      "Processing /Volumes/Server/SHARED/Corpora/Universal_Dependencies/2025_InformationTheory_Project/Universal Dependencies 2.15/ud-treebanks-v2.15/UD_Chinese-GSDSimp/zh_gsdsimp-ud-train.conllu...\n",
      "Total sentences processed: 4997\n",
      "CSV file saved as /Volumes/Server/SHARED/Corpora/Universal_Dependencies/2025_InformationTheory_Project/chinese_noun_mw.csv\n",
      "Found 8128 unique nouns with 10136 noun-measure word pairs\n"
     ]
    }
   ],
   "source": [
    "# ## 7. Processing All Files and Generating CSV\n",
    "\n",
    "# %%\n",
    "def write_to_csv(noun_mw_data, output_file):\n",
    "    \"\"\"\n",
    "    Writes the noun-measure word data to a CSV file.\n",
    "    \n",
    "    Input:\n",
    "        - noun_mw_data (defaultdict): The nested dictionary of noun-measure word pairs and counts\n",
    "        - output_file (string): Path to the output CSV file\n",
    "    \n",
    "    Output: None (writes to a file on disk)\n",
    "    \"\"\"\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['Noun', 'MW', 'Count_Pre', 'Count_Post'])\n",
    "        \n",
    "        for noun in sorted(noun_mw_data.keys()):\n",
    "            for mw in sorted(noun_mw_data[noun].keys()):\n",
    "                writer.writerow([\n",
    "                    noun,\n",
    "                    mw,\n",
    "                    noun_mw_data[noun][mw]['pre'],\n",
    "                    noun_mw_data[noun][mw]['post']\n",
    "                ])\n",
    "    \n",
    "    print(f\"CSV file saved as {output_file}\")\n",
    "    \n",
    "    # Return a DataFrame version for further analysis\n",
    "    rows = []\n",
    "    for noun in sorted(noun_mw_data.keys()):\n",
    "        for mw in sorted(noun_mw_data[noun].keys()):\n",
    "            rows.append({\n",
    "                'Noun': noun,\n",
    "                'MW': mw,\n",
    "                'Count_Pre': noun_mw_data[noun][mw]['pre'],\n",
    "                'Count_Post': noun_mw_data[noun][mw]['post']\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# %%\n",
    "def process_all_chinese_files(file_paths, output_file='chinese_noun_mw.csv'):\n",
    "    \"\"\"\n",
    "    Processes all Chinese files and combines the results into a single CSV.\n",
    "    \n",
    "    Input:\n",
    "        - file_paths (list): List of file paths to CoNLL-U files\n",
    "        - output_file (string): Path for the output CSV file\n",
    "    \n",
    "    Output: DataFrame containing the noun-measure word data\n",
    "    \"\"\"\n",
    "    all_sentences = []\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        print(f\"Processing {file_path}...\")\n",
    "        sentences = parse_conllu_sentences(file_path)\n",
    "        all_sentences.extend(sentences)\n",
    "    \n",
    "    print(f\"Total sentences processed: {len(all_sentences)}\")\n",
    "    \n",
    "    noun_mw_data = collect_noun_mw_pairs(all_sentences)\n",
    "    df = write_to_csv(noun_mw_data, output_file)\n",
    "    \n",
    "    # Print some statistics\n",
    "    total_nouns = len(noun_mw_data)\n",
    "    total_pairs = sum(len(mws) for mws in noun_mw_data.values())\n",
    "    print(f\"Found {total_nouns} unique nouns with {total_pairs} noun-measure word pairs\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# %%\n",
    "# Process all Chinese files and generate the CSV\n",
    "# Note: This may take some time depending on the corpus size\n",
    "conllu_files = [f for f in chinese_files if f.endswith('.conllu')]\n",
    "df_results = process_all_chinese_files(conllu_files, OUTPUT_CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count Nouns\n",
    "\n",
    "Create and export dataframe with all nouns in the corpus and their respective counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of nouns in the corpus: 30764\n"
     ]
    }
   ],
   "source": [
    "\n",
    "all_nouns = []\n",
    "\n",
    "for sentence in all_sentences:\n",
    "    for token in sentence['tokens']:\n",
    "        # all dependency relations that are \"modifier words\" and \"function words\" \n",
    "        #(i.e. words that may be tagged as Nouns for mandarin but do not appear as nouns) \n",
    "        other_deprel = [\"csubj\", \"ccomp\", \"advcl\", \"acl\", \"advmod\", \"discourse\", \"amod\", \"aux\", \"cop\", \"mark\", \"det\", \"clf\", \"case\"]\n",
    "        \n",
    "        # If current token is a noun\n",
    "        if (token['upos'] == 'NOUN') and (token[\"deprel\"] not in other_deprel):\n",
    "            all_nouns.append(token['lemma'])\n",
    "          \n",
    "print(f\"Total number of nouns in the corpus: {len(all_nouns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique nouns in the corpus: 8013\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Noun</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>年</td>\n",
       "      <td>1038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>日</td>\n",
       "      <td>373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>人</td>\n",
       "      <td>353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>月</td>\n",
       "      <td>231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>人口</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Noun  Count\n",
       "0    年   1038\n",
       "1    日    373\n",
       "2    人    353\n",
       "3    月    231\n",
       "4   人口    145"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_nouns_df = pd.DataFrame(all_nouns, columns=['Noun'])\n",
    "\n",
    "all_nouns_df = all_nouns_df['Noun'].value_counts().reset_index()\n",
    "all_nouns_df.columns = ['Noun', 'Count']\n",
    "\n",
    "print(f\"Total number of unique nouns in the corpus: {len(all_nouns_df.index)}\")\n",
    "all_nouns_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nouns_df.to_csv(\"output/all_UD-Mandarin-corpus_nouns.csv\", header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
